# Sales Data Analysis Pipeline

This project implements a sophisticated sales data analysis pipeline deployed on Databricks. The pipeline is fully automated using Apache Airflow, orchestrating the ingestion and processing of data from multiple sources. It performs comprehensive analyses to derive actionable insights aimed at expanding the customer base and enhancing customer retention.

## Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Data Sources](#data-sources)
- [Analysis](#analysis)
- [Usage](#usage)
- [Contributing](#contributing)
- [License](#license)

## Overview
The pipeline leverages Databricks for scalable data processing and Apache Airflow for workflow automation. It integrates data from Amazon S3 and Delta Lake tables, executing various analytical tasks to generate insights that support strategic business decisions.

## Architecture
The pipeline architecture includes:

1. **Data Ingestion**: Extracts data from S3 and Delta Lake tables.
2. **Data Processing**: Transforms and analyzes the ingested data.
3. **Data Storage**: Stores the processed data back into Delta Lake tables for further analysis and reporting.

## Data Sources
- **S3**: Contains a transactions CSV file with the following schema:
  - `transaction_id`
  - `customer_id`
  - `product_name`
  - `transaction_date`
  
- **Delta Lake Tables**:
  - **customers_data**: Schema includes:
    - `customer_id`
    - `customer_name`
    - `join_date`
    - `location`
    
  - **products_data**: Schema includes:
    - `product_id`
    - `category`
    - `name`
    - `price`

## Analysis
The pipeline performs the following analyses:

- Identification of customers who have purchased only one item from Apple.
- Detection of customers who made multiple item purchases on their first transaction.
- Calculation of the average delay between consecutive purchases for each customer.
- Computation of total revenue generated by each product.
- Analysis of products purchased after the initial transaction by each customer.

These analyses provide critical insights to drive customer acquisition and retention strategies.

## Usage
1. Start the Apache Airflow scheduler and web server.
2. Trigger the data pipeline DAG from the Airflow UI.
3. Monitor the pipeline execution and view the analysis results in Databricks.

## Contributing
Contributions are welcome! Please open an issue or submit a pull request.

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.
