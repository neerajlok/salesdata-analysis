{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e701afa-f0d6-4f96-aa8f-3040aa359437",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,lead,broadcast,collect_list,size,row_number,count,datediff,sum\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb41fe5b-6468-499f-b36e-cf656b4577f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", \"<--Your keys-->\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", \"<--Your keys-->\")\n",
    "spark=SparkSession.builder.appName('anlysis_app').getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82d53e68-98e8-484c-870f-1c5333a43c8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|customer_id|customer_name|location|\n",
      "+-----------+-------------+--------+\n",
      "|        105|          Eva|    Ohio|\n",
      "|        108|        Henry|    Utah|\n",
      "+-----------+-------------+--------+\n",
      "\n",
      "The Partitioned data has been written in DBFS\n"
     ]
    }
   ],
   "source": [
    "# checking which customer bought airpods after iphone\n",
    "\n",
    "transactions_df1 = spark.read.csv(\"<--S3 path-->\",header=True,inferSchema=True)\n",
    "customers_df = spark.read.table('customers_data')\n",
    "\n",
    "# Perform the join and select relevant columns\n",
    "df1_a = transactions_df1 \\\n",
    "    .join(broadcast(customers_df), 'customer_id', 'inner') \\\n",
    "    .select(\n",
    "        transactions_df1.transaction_id,\n",
    "        transactions_df1.customer_id,\n",
    "        customers_df.customer_name,\n",
    "        col('product_name').alias('first_purchase'),\n",
    "        transactions_df1.transaction_date,\n",
    "        customers_df.location\n",
    "    )\n",
    "\n",
    "# Defining the windows specifications\n",
    "w = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "# dtermining which customer bought aipods after iphone\n",
    "result_df1 = df1_a \\\n",
    "    .withColumn('next_purchase', lead('first_purchase').over(w)) \\\n",
    "    .filter((col('first_purchase') == 'iPhone') & (col('next_purchase') == 'AirPods')) \\\n",
    "    .select('customer_id','customer_name','location')\n",
    "\n",
    "result_df1.show()\n",
    "\n",
    "#writing the result in parquet format\n",
    "dbfs_path='dbfs:/FileStore/output/first_transform'\n",
    "result_df1.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('location') \\\n",
    "    .format('parquet') \\\n",
    "    .save(dbfs_path)\n",
    "print(f'The Partitioned data has been written in DBFS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44b3ee0-3573-4516-8a1a-e76f5343fc99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------------+-------------+\n",
      "|customer_id|customer_name|products_purchased|     location|\n",
      "+-----------+-------------+------------------+-------------+\n",
      "|        139|        Amber|          [iPhone]|     Nebraska|\n",
      "|        140|      Michael|         [AirPods]|     Virginia|\n",
      "|        141|      Barbara|         [MacBook]|      Alabama|\n",
      "|        142|  Christopher|          [iPhone]|   Washington|\n",
      "|        143|      Barbara|         [AirPods]|     Kentucky|\n",
      "|        144|      Theresa|         [MacBook]|  Mississippi|\n",
      "|        145|      Kenneth|          [iPhone]|   Washington|\n",
      "|        146|       Martin|         [AirPods]|         Utah|\n",
      "|        147|        Molly|         [MacBook]|Massachusetts|\n",
      "|        148|         Ryan|          [iPhone]|     Colorado|\n",
      "|        149|       George|         [AirPods]| North Dakota|\n",
      "|        150|  Christopher|         [MacBook]| Rhode Island|\n",
      "|        151|      Stephen|          [iPhone]|         Ohio|\n",
      "|        152|       Robert|         [AirPods]|       Hawaii|\n",
      "|        153|       Justin|         [MacBook]|     Colorado|\n",
      "|        154|      Carolyn|          [iPhone]|    Louisiana|\n",
      "+-----------+-------------+------------------+-------------+\n",
      "\n",
      "The Partitioned data has been written in DBFS\n"
     ]
    }
   ],
   "source": [
    "# customers who have bought only 1 item from apple \n",
    "\n",
    "df2a = transactions_df1.groupBy('customer_id') \\\n",
    "    .agg(collect_list('product_name').alias('products_purchased')) \\\n",
    "    .filter(size('products_purchased') == 1)\n",
    "\n",
    "# Join with customers_df to get customer details\n",
    "result_df2 = customers_df.join(broadcast(df2a), 'customer_id', 'inner') \\\n",
    "    .select('customer_id', 'customer_name', 'products_purchased', 'location')\n",
    "\n",
    "\n",
    "result_df2.show()\n",
    "\n",
    "\n",
    "dbfs_path = 'dbfs:/FileStore/output/second_transform'\n",
    "\n",
    "#writing the result in parquet format\n",
    "\n",
    "result_df2.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy('location') \\\n",
    "    .format('parquet') \\\n",
    "    .save(dbfs_path)\n",
    "\n",
    "print('The Partitioned data has been written in DBFS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15eedabb-2c29-4941-8725-71ca1ac3a015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------------+----------+\n",
      "|customer_id|transaction_date|items_bought_together|desc_order|\n",
      "+-----------+----------------+---------------------+----------+\n",
      "+-----------+----------------+---------------------+----------+\n",
      "\n",
      "Number of people who have purchased multiple items on their first purchase: 0\n"
     ]
    }
   ],
   "source": [
    "# customers who have bought multiple iteams together on their first purchase\n",
    "w1 = Window.partitionBy(\"customer_id\", \"transaction_date\").orderBy(\"transaction_date\")\n",
    "\n",
    "# Determining items bought together\n",
    "intermediate_df = transactions_df1 \\\n",
    "    .withColumn('items_bought_together', collect_list('product_name').over(w1)) \\\n",
    "    .select('customer_id', 'transaction_date', 'items_bought_together')\n",
    "\n",
    "# Defining the window specification\n",
    "w2 = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "# Add row number and filter\n",
    "result_df3 = intermediate_df.withColumn(\"desc_order\", row_number().over(w2)) \\\n",
    "    .filter((col(\"desc_order\") == 1) & (size(col('items_bought_together')) > 1))\n",
    "\n",
    "# Show the result\n",
    "result_df3.show()\n",
    "print(f'Number of people who have purchased multiple items on their first purchase: {result_df3.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ebe313d-5dc4-4b54-b004-65f0ef393d44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------------+\n",
      "|customer_id|customer_name|   avg(delay_days)|\n",
      "+-----------+-------------+------------------+\n",
      "|        105|          Eva|18.666666666666668|\n",
      "|        106|        Frank|18.666666666666668|\n",
      "|        107|        Grace|              28.0|\n",
      "|        108|        Henry|              27.5|\n",
      "|        109|       Summer|              50.0|\n",
      "|        110|      Michael|              50.0|\n",
      "|        111|        James|              50.0|\n",
      "|        112|     Kimberly|              50.0|\n",
      "|        113|      Lindsay|              50.0|\n",
      "|        114|          Amy|              50.0|\n",
      "|        115|        Brian|              50.0|\n",
      "|        116|      Vanessa|              50.0|\n",
      "|        117|        Steve|              50.0|\n",
      "|        118|         Tina|              50.0|\n",
      "|        119|       Parker|              50.0|\n",
      "|        120|       Thomas|              50.0|\n",
      "|        121|         Jane|              50.0|\n",
      "|        122|      Melanie|              50.0|\n",
      "|        123|        Roger|              50.0|\n",
      "|        124|        Derek|              50.0|\n",
      "+-----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The data has been written in DBFS\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df4_a = transactions_df1 \\\n",
    "    .join(customers_df, 'customer_id') \\\n",
    "    .select('customer_id', 'customer_name', 'product_name', 'transaction_date')\n",
    "\n",
    "# Define the window specifications\n",
    "w1 = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "w2 = Window.partitionBy(\"customer_id\")\n",
    "\n",
    "# Calculate the next transaction and count transactions per customer\n",
    "df4_a = df4_a \\\n",
    "    .withColumn('next_transaction', lead('transaction_date').over(w1)) \\\n",
    "    .withColumn('transaction_count', count('customer_id').over(w2)) \\\n",
    "    .filter((col('next_transaction').isNotNull()) & (col('transaction_count') >= 2)) \\\n",
    "    .withColumn('delay_days', datediff(col('next_transaction'), col('transaction_date')))\n",
    "\n",
    "# Calculate the average delay time\n",
    "result_df4 = df4_a.groupBy('customer_id', 'customer_name').agg({'delay_days': 'avg'})\n",
    "\n",
    "# Show the result\n",
    "result_df4.show()\n",
    "\n",
    "dbfs_path='dbfs:/FileStore/output/fourth_transform'\n",
    "result_df4.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .format('parquet') \\\n",
    "    .save(dbfs_path)\n",
    "print(f'The data has been written in DBFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe75b1c-f20f-4934-b08c-e62ea3f7d61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------+\n",
      "|  category|product_name|revenue|\n",
      "+----------+------------+-------+\n",
      "|    Laptop|     MacBook|42000.0|\n",
      "|Smartphone|      iPhone|21700.0|\n",
      "| Accessory|     AirPods| 7750.0|\n",
      "+----------+------------+-------+\n",
      "\n",
      "The data has been written in DBFS\n"
     ]
    }
   ],
   "source": [
    "#total revenue generated by each product\n",
    "products_df = spark.read.table('products_csv')\n",
    "products_df.join(transactions_df1,'product_name').groupBy('category','product_name').agg(sum('price').alias('revenue')).sort(col('revenue').desc()).show()\n",
    "result_df5=products_df.join(transactions_df1,'product_name').groupBy('category','product_name').agg(sum('price').alias('revenue')).sort(col('revenue').desc())\n",
    "dbfs_path='dbfs:/FileStore/output/fifth_transform'\n",
    "result_df5.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .format('parquet') \\\n",
    "    .save(dbfs_path)\n",
    "print(f'The data has been written in DBFS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20ef590-0937-425c-a554-bc1e9e517e81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------+-------------------------------+\n",
      "|customer_id|customer_name|location      |products_after_initial_purchase|\n",
      "+-----------+-------------+--------------+-------------------------------+\n",
      "|105        |Eva          |Ohio          |[AirPods, MacBook, AirPods]    |\n",
      "|106        |Frank        |Nevada        |[MacBook, AirPods, MacBook]    |\n",
      "|107        |Grace        |Colorado      |[iPhone, iPhone]               |\n",
      "|108        |Henry        |Utah          |[AirPods, AirPods]             |\n",
      "|109        |Summer       |Minnesota     |[MacBook]                      |\n",
      "|110        |Michael      |Tennessee     |[iPhone]                       |\n",
      "|111        |James        |Michigan      |[AirPods]                      |\n",
      "|112        |Kimberly     |Vermont       |[MacBook]                      |\n",
      "|113        |Lindsay      |New Jersey    |[iPhone]                       |\n",
      "|114        |Amy          |Connecticut   |[AirPods]                      |\n",
      "|115        |Brian        |South Carolina|[MacBook]                      |\n",
      "|116        |Vanessa      |Kentucky      |[iPhone]                       |\n",
      "|117        |Steve        |Indiana       |[AirPods]                      |\n",
      "|118        |Tina         |Indiana       |[MacBook]                      |\n",
      "|119        |Parker       |Wisconsin     |[iPhone]                       |\n",
      "|120        |Thomas       |Kansas        |[AirPods]                      |\n",
      "|121        |Jane         |New Jersey    |[MacBook]                      |\n",
      "|122        |Melanie      |Arkansas      |[iPhone]                       |\n",
      "|123        |Roger        |Hawaii        |[AirPods]                      |\n",
      "|124        |Derek        |New Hampshire |[MacBook]                      |\n",
      "+-----------+-------------+--------------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "The data has been written in DBFS\n"
     ]
    }
   ],
   "source": [
    "w = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "# Add a row number to identify the first purchase\n",
    "transactions_with_row_number = transactions_df1 \\\n",
    "    .withColumn('row_num', row_number().over(w))\n",
    "\n",
    "# Filter to get all rows where row_num is greater than 1 (i.e., after the first purchase)\n",
    "products_after_initial_purchase = transactions_with_row_number \\\n",
    "    .filter(col('row_num') > 1) \\\n",
    "    .select('customer_id', 'product_name')\n",
    "\n",
    "# Group by customer_id and collect all products bought after the initial purchase into a list\n",
    "products_aggregated = products_after_initial_purchase \\\n",
    "    .groupBy('customer_id') \\\n",
    "    .agg(collect_list('product_name').alias('products_after_initial_purchase'))\n",
    "\n",
    "# Join the aggregated DataFrame with the customers DataFrame\n",
    "result_df6 = products_aggregated \\\n",
    "    .join(customers_df, 'customer_id', 'inner') \\\n",
    "    .select('customer_id', 'customer_name','location', 'products_after_initial_purchase')\n",
    "\n",
    "# Show the result\n",
    "result_df6.show(truncate=False)\n",
    "\n",
    "dbfs_path='dbfs:/FileStore/output/sixth_transform'\n",
    "result_df6.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .format('parquet') \\\n",
    "    .save(dbfs_path)\n",
    "print(f'The data has been written in DBFS')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "apple_analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
